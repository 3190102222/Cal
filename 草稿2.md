假设我们得到了统计量$T_{i}, \ i \in \{1,2,\dots,n\}$的直方图。对于截断参数$\lambda$，我们利用截尾方法获得对稀疏性$\pi$的估计值$\hat{\pi}$。记$W_{0}(\lambda) = \#\{i:H_{i} = H_{0i}, \ T_{i}>\lambda\}$。我们有：
$$
    \mathbb{E}W_{0}(\lambda) = \mathbb{E}\sum\limits_{ H_{i} \ is \ true }I[T_{i}>\lambda ] = \mathbb{E}[n_{0}P_{0}(T >\lambda)|n_{0}]  = n(1-\pi)P_{0}(T>\lambda)   
$$
我们期望$W_{0}(\lambda) \approx W(\lambda)$，基于我们的假设——来自于非零假设的统计量$T_{i}$几乎都要小于截断参数$\lambda$。因此，我们用$W_{\lambda} = \#\{i:T_{i} > \lambda \}$替换$W_{0}(\lambda)$，得到稀疏性的估计值。为了方便，记$p = P_{0}(T > \lambda), \ W =W(\lambda), \ W_{0}=W_{0}(\lambda)$。
$$
     W(\lambda)  = np(1-\hat{\pi}) \approx n p(1-\pi) 

$$
接下来，我们会给出计算估计值$\hat{\pi}$均方误差（$MSE$）的公式。
$$
     \hat{\pi} - \pi = \dfrac{W - \mathbb{E}W_{0}}{np} = \dfrac{W_{0}-\mathbb{E}W_{0}}{np} + \dfrac{W_{1}}{np}  
$$
$$
     \mathbb{E}(\hat{\pi}-\pi)^2 = \mathbb{E} \left(\dfrac{W_{0}-\mathbb{E}W_{0}}{np}\right)^2 + 2 \mathbb{E}\left(\dfrac{W_{0}-\mathbb{E}W_{0}}{np}\right)\left( \dfrac{W_{1}}{np}\right) + \mathbb{E}\left( \dfrac{W_{1}}{np}\right)^2
$$
$W_{0}, \ W_{1}$是不相关的：
$$
 \mathbb{E}W_{0}W_{1}=  \mathbb{E}\sum\limits_{ H_{i} \ is \ true }I[T_{i}>\lambda ] \sum\limits_{ H_{i} \ is \ non-null }I[T_{i}>\lambda ] = \mathbb{E}[n_{0}P_{0}(T >\lambda)(n-n_{0})P_{1}(T >\lambda)|n_{0}] = n(1-\pi)P_{0}(T >\lambda)n\pi P_{1}(T >\lambda) = \mathbb{E}W_{0} \cdot \mathbb{E}W_{1}
$$
因此，
$$
       \mathbb{E}(\hat{\pi}-\pi)^2 = \mathbb{E} \left(\dfrac{W_{0}-\mathbb{E}W_{0}}{np}\right)^2 +\mathbb{E}\left( \dfrac{W_{1}}{np}\right)^2 = \dfrac{1}{(np)^2} \{Var(W_{0}) + \mathbb{E}W_{1}^2\}
$$
